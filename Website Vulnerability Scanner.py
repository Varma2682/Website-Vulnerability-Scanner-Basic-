import requests

def check_security_header(header_name, response):
  """
  This function checks for a specific security header in the response.

  Args:
    header_name (str): The name of the security header to check.
    response (requests.Response): The response object from the website request.

  Returns:
    bool: True if the header is present and not empty, False otherwise.
  """
  return header_name in response.headers and response.headers[header_name]

def check_security_headers(url):
  """
  This function checks a website for missing security headers.

  Args:
    url (str): The URL of the website to scan.

  Returns:
    list: A list of missing security headers.
  """
  response = requests.get(url)
  missing_headers = []

  headers_to_check = {
    "X-Frame-Options": True,
    "X-XSS-Protection": True,
    "Content-Security-Policy": False  # May not be present on all sites
  }

  for header, optional in headers_to_check.items():
    if not check_security_header(header, response):
      missing_headers.append(header)

  return missing_headers

def parse_robots_txt(robots_txt_content):
  """
  This function parses the content of a robots.txt file and identifies basic weaknesses.

  Args:
    robots_txt_content (str): The content of the robots.txt file.

  Returns:
    str: A message indicating any robots.txt issues, or "No issues found (basic check)" otherwise.
  """
  if not robots_txt_content:
    return "robots.txt might be empty."
  elif "Disallow: /" not in robots_txt_content:
    return "robots.txt might allow access to all directories."
  else:
    return "No issues found with robots.txt (basic check)."

def check_robots_txt(url):
  """
  This function checks for a robots.txt file and potential weaknesses.

  Args:
    url (str): The URL of the website to scan.

  Returns:
    str: A message indicating any robots.txt issues.
  """
  robots_url = url + "/robots.txt"
  try:
    response = requests.get(robots_url)
    if response.status_code == 200:
      return parse_robots_txt(response.text)
    else:
      return f"robots.txt request failed with status code: {response.status_code}"
  except requests.exceptions.RequestException as e:
    return f"Failed to access robots.txt: {e}"

if __name__ == "__main__":
  # Get website URL from user
  target_url = input("Enter the website URL to scan: ")

  # Check for missing security headers
  missing_headers = check_security_headers(target_url)
  if missing_headers:
    print(f"** Missing security headers:")
    for header in missing_headers:
      print(f"- {header}")
  else:
    print("All checked security
